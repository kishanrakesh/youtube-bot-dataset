{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Bot Classifier - Train on GPU\n",
    "\n",
    "**Google Colab Setup Steps:**\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Select \"T4 GPU\" ‚Üí Save\n",
    "2. **Run cells sequentially** from top to bottom\n",
    "3. **Upload dataset.zip** when prompted (345MB - may take 2-3 minutes)\n",
    "4. **Configure experiment** in the config cell (run single model or compare multiple)\n",
    "5. **Wait for training** (~15 minutes per model on T4 GPU)\n",
    "6. **Download results** at the end\n",
    "\n",
    "**Estimated Time:**\n",
    "- Single model: ~15-20 minutes\n",
    "- All 4 models (comparison): ~60-75 minutes\n",
    "\n",
    "**Free Tier Notes:**\n",
    "- Colab free tier gives ~12 hours of continuous GPU time\n",
    "- If disconnected, you'll need to re-upload dataset and restart training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Checklist\n",
    "\n",
    "Before running, verify:\n",
    "- [ ] GPU is enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- [ ] You have dataset.zip ready to upload (345MB)\n",
    "- [ ] You know which config to run (single model or comparison)\n",
    "\n",
    "**Notebook Flow:**\n",
    "1. Check GPU ‚úì\n",
    "2. Upload dataset ‚úì\n",
    "3. Extract and verify data ‚úì\n",
    "4. Configure experiment ‚úì\n",
    "5. Train model(s) ‚úì\n",
    "6. Review results ‚úì\n",
    "7. Download models and exports ‚úì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Verify CUDA is available\n",
    "import torch\n",
    "print(f\"\\nüîç GPU Check:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: GPU not detected!\")\n",
    "    print(\"   Go to: Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print('üì¶ Upload dataset.zip (345MB)')\n",
    "print('‚è±Ô∏è  This may take 2-3 minutes depending on your connection...')\n",
    "print()\n",
    "\n",
    "# Check if already uploaded\n",
    "if os.path.exists('dataset.zip'):\n",
    "    print('‚úÖ dataset.zip already exists!')\n",
    "    response = input('Re-upload? (y/n): ')\n",
    "    if response.lower() != 'y':\n",
    "        print('Skipping upload, using existing file.')\n",
    "    else:\n",
    "        uploaded = files.upload()\n",
    "else:\n",
    "    uploaded = files.upload()\n",
    "\n",
    "print('‚úÖ Upload complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists('dataset'):\n",
    "    print('üìÇ Extracting dataset.zip...')\n",
    "    !unzip -q dataset.zip\n",
    "    print('‚úÖ Extraction complete!')\n",
    "else:\n",
    "    print('‚úÖ Dataset already extracted!')\n",
    "\n",
    "print('\\nüìä Dataset structure:')\n",
    "!ls -lh dataset/train dataset/val\n",
    "\n",
    "# Count images\n",
    "import os\n",
    "train_bot = len(os.listdir('dataset/train/bot'))\n",
    "train_not_bot = len(os.listdir('dataset/train/not_bot'))\n",
    "val_bot = len(os.listdir('dataset/val/bot'))\n",
    "val_not_bot = len(os.listdir('dataset/val/not_bot'))\n",
    "\n",
    "print(f'\\nüìà Dataset summary:')\n",
    "print(f'   Training: {train_bot} bots, {train_not_bot} not_bots (total: {train_bot + train_not_bot})')\n",
    "print(f'   Validation: {val_bot} bots, {val_not_bot} not_bots (total: {val_bot + val_not_bot})')\n",
    "print(f'   Class balance: {train_bot/(train_bot + train_not_bot)*100:.1f}% bots in training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Import required libraries\n",
    "print('üìö Importing libraries...')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úÖ Libraries loaded successfully!')\n",
    "print(f'üéØ Using device: {DEVICE}')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print('üé≤ Random seeds set for reproducibility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "**Easy Mode**: Just change `SELECTED_CONFIG` below to run a single model:\n",
    "- 0 = MobileNetV2 with equal weights\n",
    "- 1 = MobileNetV2 with weighted classes\n",
    "- 2 = ResNet18 with equal weights\n",
    "- 3 = ResNet18 with weighted classes\n",
    "- 4 = ResNet34 with equal weights\n",
    "- 5 = ResNet34 with weighted classes\n",
    "- 'all' = Run all configurations and compare\n",
    "\n",
    "**Advanced Mode**: Edit the CONFIGS list to add your own configurations or tweak parameters.\n",
    "\n",
    "**Model Info:**\n",
    "- **MobileNetV2**: Lightweight, fast training (~12-15 min/config)\n",
    "- **ResNet18**: Small ResNet, good balance (~15-18 min/config)\n",
    "- **ResNet34**: Deeper ResNet, potentially more accurate (~18-22 min/config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE YOUR EXPERIMENT HERE\n",
    "# ============================================================================\n",
    "\n",
    "# Which config to run? Set to a number (0-5) or 'all'\n",
    "SELECTED_CONFIG = 0  # Change this to: 0, 1, 2, 3, 4, 5, or 'all'\n",
    "\n",
    "# Define all available configurations\n",
    "CONFIGS = [\n",
    "    # Config 0: MobileNetV2 with equal weights\n",
    "    {\n",
    "        'name': 'mobilenet_equal',\n",
    "        'model': 'mobilenet_v2',\n",
    "        'class_weight_bot': 1.0,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "    },\n",
    "    # Config 1: MobileNetV2 with weighted classes (prioritize bot detection)\n",
    "    {\n",
    "        'name': 'mobilenet_weighted',\n",
    "        'model': 'mobilenet_v2',\n",
    "        'class_weight_bot': 1.65,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "    },\n",
    "    # Config 2: ResNet18 with equal weights\n",
    "    {\n",
    "        'name': 'resnet18_equal',\n",
    "        'model': 'resnet18',\n",
    "        'class_weight_bot': 1.0,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "    },\n",
    "    # Config 3: ResNet18 with weighted classes (prioritize bot detection)\n",
    "    {\n",
    "        'name': 'resnet18_weighted',\n",
    "        'model': 'resnet18',\n",
    "        'class_weight_bot': 1.65,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "    },\n",
    "    # Config 4: ResNet34 with equal weights\n",
    "    {\n",
    "        'name': 'resnet34_equal',\n",
    "        'model': 'resnet34',\n",
    "        'class_weight_bot': 1.0,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "    },\n",
    "    # Config 5: ResNet34 with weighted classes (prioritize bot detection)\n",
    "    {\n",
    "        'name': 'resnet34_weighted',\n",
    "        'model': 'resnet34',\n",
    "        'class_weight_bot': 1.65,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "    },\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# END CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Determine which configs to run\n",
    "if SELECTED_CONFIG == 'all':\n",
    "    configs_to_run = CONFIGS\n",
    "    print(f\"üîÑ Running ALL {len(CONFIGS)} configurations for comparison:\")\n",
    "else:\n",
    "    configs_to_run = [CONFIGS[SELECTED_CONFIG]]\n",
    "    print(f\"üéØ Running SINGLE configuration:\")\n",
    "\n",
    "for i, cfg in enumerate(configs_to_run):\n",
    "    print(f\"  - {cfg['name']}: {cfg['model']}, bot_weight={cfg['class_weight_bot']}, epochs={cfg['num_epochs']}\")\n",
    "\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvatarDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for label_dir in ['bot', 'not_bot']:\n",
    "            label = 0 if label_dir == 'bot' else 1\n",
    "            for img_path in (self.root_dir / label_dir).glob('*.png'):\n",
    "                self.samples.append((str(img_path), label))\n",
    "        bot_count = sum(1 for _, l in self.samples if l == 0)\n",
    "        print(f'{root_dir}: {len(self.samples)} samples (bot={bot_count}, not_bot={len(self.samples)-bot_count})')\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        return self.transform(image) if self.transform else image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = AvatarDataset('dataset/train', train_transform)\n",
    "test_dataset = AvatarDataset('dataset/val', test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    loss_sum, correct, total = 0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    return loss_sum / len(loader), 100. * correct / total\n",
    "\n",
    "def test_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_sum, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_sum += criterion(outputs, labels).item()\n",
    "            probs = torch.softmax(outputs, 1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    return (loss_sum / len(loader), 100. * correct / total,\n",
    "            np.array(all_preds), np.array(all_labels), np.array(all_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selected configuration(s)\n",
    "import time\n",
    "\n",
    "for config_idx, config in enumerate(configs_to_run):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING CONFIG: {config['name']} ({config_idx+1}/{len(configs_to_run)})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    model_name = config['model']\n",
    "    class_weight_bot = config['class_weight_bot']\n",
    "    lr = config['learning_rate']\n",
    "    epochs = config['num_epochs']\n",
    "    config_name = config['name']\n",
    "    \n",
    "    print(f\"üìã Configuration:\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Class weight (bot): {class_weight_bot}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print()\n",
    "    \n",
    "    # Build model\n",
    "    if model_name == 'mobilenet_v2':\n",
    "        model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
    "    elif model_name == 'resnet18':\n",
    "        model = torchvision.models.resnet18(pretrained=True)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, 2)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = torchvision.models.resnet34(pretrained=True)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Setup training\n",
    "    class_weights = torch.tensor([class_weight_bot, 1.0]).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)\n",
    "    \n",
    "    best_recall = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'bot_recall': []}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"üèãÔ∏è Training started...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        test_loss, test_acc, preds, labels, probs = test_epoch(model, test_loader, criterion, DEVICE)\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        bot_recall = cm[0,0] / (cm[0,0] + cm[0,1]) if (cm[0,0] + cm[0,1]) > 0 else 0\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | Train: {train_acc:5.2f}% | Test: {test_acc:5.2f}% | Bot Recall: {bot_recall:.4f}', end='')\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['bot_recall'].append(bot_recall)\n",
    "        \n",
    "        scheduler.step(bot_recall)\n",
    "        \n",
    "        if bot_recall > best_recall:\n",
    "            best_recall = bot_recall\n",
    "            model_filename = f'{config_name}.pth'\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(' ‚úì [BEST]')\n",
    "        else:\n",
    "            print()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(f\"\\nüìä Evaluating best model...\")\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    test_loss, test_acc, preds, labels, probs = test_epoch(model, test_loader, criterion, DEVICE)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    bot_recall = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    bot_precision = cm[0,0] / (cm[0,0] + cm[1,0])\n",
    "    bot_f1 = 2 * bot_precision * bot_recall / (bot_precision + bot_recall)\n",
    "    roc_auc = roc_auc_score(labels, probs[:, 0])\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\n‚úÖ RESULTS for {config_name}:\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"   Bot Recall: {bot_recall:.4f} (catches {bot_recall*100:.1f}% of bots)\")\n",
    "    print(f\"   Bot Precision: {bot_precision:.4f} ({bot_precision*100:.1f}% accurate when predicting bot)\")\n",
    "    print(f\"   Bot F1 Score: {bot_f1:.4f}\")\n",
    "    print(f\"   ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"   Training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"\\n   Confusion Matrix:\")\n",
    "    print(f\"   {cm}\")\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'config_name': config_name,\n",
    "        'model': model_name,\n",
    "        'class_weight_bot': class_weight_bot,\n",
    "        'test_accuracy': test_acc,\n",
    "        'bot_recall': bot_recall,\n",
    "        'bot_precision': bot_precision,\n",
    "        'bot_f1': bot_f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'training_time': training_time,\n",
    "        'confusion_matrix': cm,\n",
    "        'history': history,\n",
    "    }\n",
    "    all_results.append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if len(configs_to_run) == 1:\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "else:\n",
    "    print(f\"‚úÖ ALL {len(configs_to_run)} TRAININGS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary and comparison\n",
    "import pandas as pd\n",
    "\n",
    "if len(all_results) > 1:\n",
    "    # COMPARISON MODE - Multiple configs were run\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS - COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for result in all_results:\n",
    "        comparison_data.append({\n",
    "            'Config': result['config_name'],\n",
    "            'Model': result['model'],\n",
    "            'Bot Weight': result['class_weight_bot'],\n",
    "            'Test Acc (%)': f\"{result['test_accuracy']:.2f}\",\n",
    "            'Bot Recall': f\"{result['bot_recall']:.4f}\",\n",
    "            'Bot Precision': f\"{result['bot_precision']:.4f}\",\n",
    "            'Bot F1': f\"{result['bot_f1']:.4f}\",\n",
    "            'ROC AUC': f\"{result['roc_auc']:.4f}\",\n",
    "            'Time (min)': f\"{result['training_time']/60:.1f}\",\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Find best configuration for bot recall\n",
    "    best_idx = max(range(len(all_results)), key=lambda i: all_results[i]['bot_recall'])\n",
    "    best_result = all_results[best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ BEST CONFIGURATION (by Bot Recall)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Config: {best_result['config_name']}\")\n",
    "    print(f\"Model: {best_result['model']}\")\n",
    "    print(f\"Bot Class Weight: {best_result['class_weight_bot']}\")\n",
    "    print(f\"Bot Recall: {best_result['bot_recall']:.4f}\")\n",
    "    print(f\"Bot Precision: {best_result['bot_precision']:.4f}\")\n",
    "    print(f\"Bot F1 Score: {best_result['bot_f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {best_result['roc_auc']:.4f}\")\n",
    "    print(f\"Test Accuracy: {best_result['test_accuracy']:.2f}%\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(best_result['confusion_matrix'])\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    # SINGLE MODE - Only one config was run\n",
    "    result = all_results[0]\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RESULTS: {result['config_name']}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model: {result['model']}\")\n",
    "    print(f\"Bot Class Weight: {result['class_weight_bot']}\")\n",
    "    print(f\"\\nüìà Metrics:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Bot Recall: {result['bot_recall']:.4f}\")\n",
    "    print(f\"  Bot Precision: {result['bot_precision']:.4f}\")\n",
    "    print(f\"  Bot F1 Score: {result['bot_f1']:.4f}\")\n",
    "    print(f\"  ROC AUC: {result['roc_auc']:.4f}\")\n",
    "    print(f\"  Training Time: {result['training_time']/60:.1f} minutes\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(result['confusion_matrix'])\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_result = result  # For use in later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "if len(all_results) > 1:\n",
    "    # COMPARISON MODE - Multiple configs\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Bot Recall comparison (bar chart)\n",
    "    config_names = [r['config_name'] for r in all_results]\n",
    "    bot_recalls = [r['bot_recall'] for r in all_results]\n",
    "    \n",
    "    # Color by model type\n",
    "    colors = []\n",
    "    for name in config_names:\n",
    "        if 'mobilenet' in name:\n",
    "            colors.append('blue')\n",
    "        elif 'resnet34' in name:\n",
    "            colors.append('red')\n",
    "        elif 'resnet18' in name:\n",
    "            colors.append('green')\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "    \n",
    "    axes[0, 0].bar(range(len(config_names)), bot_recalls, color=colors, alpha=0.7)\n",
    "    axes[0, 0].set_xticks(range(len(config_names)))\n",
    "    axes[0, 0].set_xticklabels(config_names, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0, 0].set_ylabel('Bot Recall')\n",
    "    axes[0, 0].set_title('Bot Recall Comparison')\n",
    "    axes[0, 0].grid(True, axis='y')\n",
    "    axes[0, 0].axhline(y=max(bot_recalls), color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 2: F1 Score comparison\n",
    "    bot_f1s = [r['bot_f1'] for r in all_results]\n",
    "    axes[0, 1].bar(range(len(config_names)), bot_f1s, color=colors, alpha=0.7)\n",
    "    axes[0, 1].set_xticks(range(len(config_names)))\n",
    "    axes[0, 1].set_xticklabels(config_names, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0, 1].set_ylabel('Bot F1 Score')\n",
    "    axes[0, 1].set_title('Bot F1 Score Comparison')\n",
    "    axes[0, 1].grid(True, axis='y')\n",
    "    \n",
    "    # Plot 3: Recall vs Precision scatter\n",
    "    bot_precisions = [r['bot_precision'] for r in all_results]\n",
    "    for i, (recall, precision, name) in enumerate(zip(bot_recalls, bot_precisions, config_names)):\n",
    "        axes[1, 0].scatter(recall, precision, s=200, alpha=0.7, color=colors[i], label=name)\n",
    "    axes[1, 0].set_xlabel('Bot Recall')\n",
    "    axes[1, 0].set_ylabel('Bot Precision')\n",
    "    axes[1, 0].set_title('Recall vs Precision Trade-off')\n",
    "    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=7)\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Plot 4: Training curves for best model\n",
    "    best_history = best_result['history']\n",
    "    ax1 = axes[1, 1]\n",
    "    ax1.plot(best_history['bot_recall'], marker='o', color='green', linewidth=2, label='Bot Recall')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Bot Recall', color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    ax1.set_title(f'Best Model Training: {best_result[\"config_name\"]}')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(best_history['test_acc'], marker='s', color='blue', linewidth=2, label='Test Accuracy')\n",
    "    ax2.set_ylabel('Test Accuracy (%)', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hyperparameter_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Comparison plots saved to hyperparameter_comparison.png\")\n",
    "\n",
    "else:\n",
    "    # SINGLE MODE - Training curves for single config\n",
    "    result = all_results[0]\n",
    "    history = result['history']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0].plot(history['train_loss'], marker='o', label='Train', linewidth=2)\n",
    "    axes[0].plot(history['test_loss'], marker='s', label='Test', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss over Epochs')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    axes[1].plot(history['train_acc'], marker='o', label='Train', linewidth=2)\n",
    "    axes[1].plot(history['test_acc'], marker='s', label='Test', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Accuracy over Epochs')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Plot 3: Bot Recall\n",
    "    axes[2].plot(history['bot_recall'], marker='o', color='green', linewidth=2)\n",
    "    axes[2].axhline(y=result['bot_recall'], color='r', linestyle='--', \n",
    "                    label=f'Best: {result[\"bot_recall\"]:.4f}')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Bot Recall')\n",
    "    axes[2].set_title('Bot Recall over Epochs')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_{result[\"config_name\"]}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Training plots saved to training_{result['config_name']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "if len(all_results) > 1:\n",
    "    # Download best model from comparison\n",
    "    best_model_filename = f\"{best_result['config_name']}.pth\"\n",
    "    print(f\"üì• Downloading best model: {best_model_filename}\")\n",
    "    files.download(best_model_filename)\n",
    "    files.download('hyperparameter_comparison.png')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Downloaded:\")\n",
    "    print(f\"   - {best_model_filename} (best performing model)\")\n",
    "    print(f\"   - hyperparameter_comparison.png (visual comparison)\")\n",
    "    print(f\"\\nüèÜ Best config was: {best_result['config_name']}\")\n",
    "    print(f\"   Upload {best_model_filename} to your VPS for inference.\")\n",
    "else:\n",
    "    # Download single model\n",
    "    result = all_results[0]\n",
    "    model_filename = f\"{result['config_name']}.pth\"\n",
    "    plot_filename = f\"training_{result['config_name']}.png\"\n",
    "    \n",
    "    print(f\"üì• Downloading model and plots...\")\n",
    "    files.download(model_filename)\n",
    "    files.download(plot_filename)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Downloaded:\")\n",
    "    print(f\"   - {model_filename} (trained model)\")\n",
    "    print(f\"   - {plot_filename} (training curves)\")\n",
    "    print(f\"\\nüìù Config: {result['config_name']}\")\n",
    "    print(f\"   Upload {model_filename} to your VPS for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to files\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamp for this experiment\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if len(all_results) > 1:\n",
    "    # COMPARISON MODE - Export comparison table\n",
    "    \n",
    "    # 1. Export to CSV (easy to open in Excel/Google Sheets)\n",
    "    csv_filename = f'results_comparison_{timestamp}.csv'\n",
    "    comparison_data = []\n",
    "    for result in all_results:\n",
    "        comparison_data.append({\n",
    "            'config_name': result['config_name'],\n",
    "            'model': result['model'],\n",
    "            'class_weight_bot': result['class_weight_bot'],\n",
    "            'test_accuracy': result['test_accuracy'],\n",
    "            'bot_recall': result['bot_recall'],\n",
    "            'bot_precision': result['bot_precision'],\n",
    "            'bot_f1': result['bot_f1'],\n",
    "            'roc_auc': result['roc_auc'],\n",
    "            'training_time_minutes': result['training_time'] / 60,\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"üìä Exported comparison to: {csv_filename}\")\n",
    "    \n",
    "    # 2. Export to JSON (includes full details like confusion matrices)\n",
    "    json_filename = f'results_full_{timestamp}.json'\n",
    "    export_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'num_configs': len(all_results),\n",
    "        'best_config': best_result['config_name'],\n",
    "        'results': []\n",
    "    }\n",
    "    \n",
    "    for result in all_results:\n",
    "        export_data['results'].append({\n",
    "            'config_name': result['config_name'],\n",
    "            'model': result['model'],\n",
    "            'class_weight_bot': result['class_weight_bot'],\n",
    "            'metrics': {\n",
    "                'test_accuracy': float(result['test_accuracy']),\n",
    "                'bot_recall': float(result['bot_recall']),\n",
    "                'bot_precision': float(result['bot_precision']),\n",
    "                'bot_f1': float(result['bot_f1']),\n",
    "                'roc_auc': float(result['roc_auc']),\n",
    "                'training_time_minutes': float(result['training_time'] / 60),\n",
    "            },\n",
    "            'confusion_matrix': result['confusion_matrix'].tolist(),\n",
    "            'history': {\n",
    "                'train_loss': [float(x) for x in result['history']['train_loss']],\n",
    "                'train_acc': [float(x) for x in result['history']['train_acc']],\n",
    "                'test_loss': [float(x) for x in result['history']['test_loss']],\n",
    "                'test_acc': [float(x) for x in result['history']['test_acc']],\n",
    "                'bot_recall': [float(x) for x in result['history']['bot_recall']],\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    print(f\"üìã Exported full results to: {json_filename}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Files created:\")\n",
    "    print(f\"   - {csv_filename} (summary table for Excel/Sheets)\")\n",
    "    print(f\"   - {json_filename} (full details including training history)\")\n",
    "    \n",
    "else:\n",
    "    # SINGLE MODE - Export single config results\n",
    "    \n",
    "    result = all_results[0]\n",
    "    \n",
    "    # 1. Export to CSV\n",
    "    csv_filename = f'results_{result[\"config_name\"]}_{timestamp}.csv'\n",
    "    result_data = pd.DataFrame([{\n",
    "        'config_name': result['config_name'],\n",
    "        'model': result['model'],\n",
    "        'class_weight_bot': result['class_weight_bot'],\n",
    "        'test_accuracy': result['test_accuracy'],\n",
    "        'bot_recall': result['bot_recall'],\n",
    "        'bot_precision': result['bot_precision'],\n",
    "        'bot_f1': result['bot_f1'],\n",
    "        'roc_auc': result['roc_auc'],\n",
    "        'training_time_minutes': result['training_time'] / 60,\n",
    "    }])\n",
    "    result_data.to_csv(csv_filename, index=False)\n",
    "    print(f\"üìä Exported results to: {csv_filename}\")\n",
    "    \n",
    "    # 2. Export to JSON (full details)\n",
    "    json_filename = f'results_{result[\"config_name\"]}_{timestamp}.json'\n",
    "    export_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'config_name': result['config_name'],\n",
    "        'model': result['model'],\n",
    "        'class_weight_bot': result['class_weight_bot'],\n",
    "        'metrics': {\n",
    "            'test_accuracy': float(result['test_accuracy']),\n",
    "            'bot_recall': float(result['bot_recall']),\n",
    "            'bot_precision': float(result['bot_precision']),\n",
    "            'bot_f1': float(result['bot_f1']),\n",
    "            'roc_auc': float(result['roc_auc']),\n",
    "            'training_time_minutes': float(result['training_time'] / 60),\n",
    "        },\n",
    "        'confusion_matrix': result['confusion_matrix'].tolist(),\n",
    "        'history': {\n",
    "            'train_loss': [float(x) for x in result['history']['train_loss']],\n",
    "            'train_acc': [float(x) for x in result['history']['train_acc']],\n",
    "            'test_loss': [float(x) for x in result['history']['test_loss']],\n",
    "            'test_acc': [float(x) for x in result['history']['test_acc']],\n",
    "            'bot_recall': [float(x) for x in result['history']['bot_recall']],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    print(f\"üìã Exported full results to: {json_filename}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Files created:\")\n",
    "    print(f\"   - {csv_filename} (metrics summary)\")\n",
    "    print(f\"   - {json_filename} (full details including training history)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download exported files (optional - downloads CSV and JSON)\n",
    "from google.colab import files\n",
    "\n",
    "# List of files to download\n",
    "export_files = []\n",
    "\n",
    "if len(all_results) > 1:\n",
    "    # Comparison mode - find the files we just created\n",
    "    import glob\n",
    "    export_files.extend(glob.glob('results_comparison_*.csv'))\n",
    "    export_files.extend(glob.glob('results_full_*.json'))\n",
    "else:\n",
    "    # Single mode\n",
    "    import glob\n",
    "    export_files.extend(glob.glob(f'results_{all_results[0][\"config_name\"]}_*.csv'))\n",
    "    export_files.extend(glob.glob(f'results_{all_results[0][\"config_name\"]}_*.json'))\n",
    "\n",
    "print(\"üì• Downloading exported result files...\")\n",
    "for filename in export_files:\n",
    "    print(f\"   Downloading: {filename}\")\n",
    "    files.download(filename)\n",
    "\n",
    "print(f\"\\n‚úÖ Downloaded {len(export_files)} result file(s)\")\n",
    "print(\"\\nüí° Tip: Keep these files to track your experiments over time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "**What to do next:**\n",
    "\n",
    "1. **Review results** in the cells above (metrics table and visualizations)\n",
    "2. **Download your model** - The best performing model will be downloaded automatically\n",
    "3. **Export results** - CSV and JSON files contain all experiment details\n",
    "4. **Save to Google Drive** (optional) - See cell below to backup everything\n",
    "\n",
    "**Using the trained model:**\n",
    "- Upload the `.pth` file to your VPS\n",
    "- Load it with: `model.load_state_dict(torch.load('model_name.pth'))`\n",
    "- Use for inference on new avatar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Save all results to Google Drive\n",
    "# Uncomment and run this cell to backup everything to your Google Drive\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "print('üìÅ Mounting Google Drive...')\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create backup folder\n",
    "backup_folder = '/content/drive/MyDrive/avatar_classifier_results'\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# Copy all result files\n",
    "print('üíæ Backing up to Google Drive...')\n",
    "files_to_backup = []\n",
    "\n",
    "# Model files\n",
    "import glob\n",
    "files_to_backup.extend(glob.glob('*.pth'))\n",
    "files_to_backup.extend(glob.glob('*.png'))\n",
    "files_to_backup.extend(glob.glob('*.csv'))\n",
    "files_to_backup.extend(glob.glob('*.json'))\n",
    "\n",
    "for file in files_to_backup:\n",
    "    if os.path.exists(file):\n",
    "        shutil.copy(file, backup_folder)\n",
    "        print(f'   ‚úì Backed up: {file}')\n",
    "\n",
    "print(f'\\n‚úÖ All files backed up to: {backup_folder}')\n",
    "print('üí° Your results are now safely stored in Google Drive!')\n",
    "\"\"\"\n",
    "\n",
    "print('‚ÑπÔ∏è  Uncomment the code above to backup results to Google Drive')\n",
    "print('   This is useful if you want to save everything permanently.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
